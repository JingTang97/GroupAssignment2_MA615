---
title: "correlaid_text_analysis"
article: "Xiang XU"
date: "November 3, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dev="pdf",fig.align  = 'center')
pacman::p_load(
  "ggplot2",
  "knitr",
  "arm",
  "data.table",
  "car",
  "rvest",
  "RCurl",
  "RTidyHTML",
  "XML",
  "htm2txt",
  "dplyr",
  "tidytext",
  "stringr",
  "tidyr",
  "scales",
  "wordcloud"
)
```


##Scraping webpages
```{r}
url1 <- "https://correlaid.org/blog/posts/understand-p-values"
url2 <- "https://correlaid.org/blog/posts/blockchain-explained"
url3 <- "https://correlaid.org/blog/posts/music-with-r"

doc1 <- gettxt(url1)
# doc11 <- getURL(url1)
# dic11 <- htmlTreeParse(doc11)
# doc1111 <- readLines(url1)
doc2 <- gettxt(url2)
doc3 <- gettxt(url3)

raw.text <- as.data.frame(rbind(cbind("doc1",doc1),
                                cbind("doc2",doc2),
                                cbind("doc3",doc3)))
colnames(raw.text)  <-  c("article", "text")  
raw.text$article <- as.character(raw.text$article)
raw.text$text <- as.character(raw.text$text)
str(raw.text)
```

##tidy text
```{r}
tidy_webtext <- raw.text %>%
  # unnest_tokens(sentence, text, token = "sentences") %>% 
  unnest_tokens(word, text)%>% 
  mutate(word = str_extract(word , "[a-z'\\s]+")) %>%
  anti_join(stop_words, by= "word") %>%
  filter(!word %in% "[\\s]+",
         !word %in% "",
         !word %in% NA)
```

#Chapter 1 
##look at single word frequency and visualize
###first at doc1
```{r}
text1_count <- tidy_webtext %>%
  filter(article == "doc1" )%>%
  count(word, sort = TRUE) 

text1_count %>%
  filter(n > 10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() 
```

###plotting and comparing the three articles
```{r,warning=FALSE, message=FALSE}
frequency <- tidy_webtext %>% 
  count(article, word) %>%
  group_by(article) %>%
  mutate(proportion = n/sum(n) )%>%
  select(-n) %>%
  spread(article, proportion) %>% 
  gather(article, proportion, 'doc2' : 'doc3') 
#fill all NA (word proporton) with zero
frequency[is.na(frequency)] <- 0
  

ggplot(frequency, aes(x = proportion, y = `doc1`, color = abs(`doc1` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = .7, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.01), low = "darkslategray4", high = "gray75") +
  facet_wrap(~ article, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "doc1", x = NULL)
  
```

```{r}
cor.test(data = frequency[frequency$article == "doc2",],
         ~ proportion + `doc1`)

cor.test(data = frequency[frequency$article == "doc3",],
         ~ proportion + `doc1`)
```

*As we saw in the plots, the word frequencies have little frequencies in three articles.*

#Chapter 2 Sentiment analysis

```{r}
afinn <- get_sentiments("afinn")
bing <- get_sentiments("bing")
nrc <- get_sentiments("nrc")
```

###bing sentiment analysis
```{r}
sentitext <-  raw.text %>%
  unnest_tokens(sentence, text, token = "sentences") %>% 
  group_by(article) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, sentence) %>%
  anti_join(stop_words, by = "word")

bing_analysis <- sentitext %>%
  inner_join(bing, by = "word") %>%
  count(article, index = linenumber , sentiment)%>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(bing_analysis, aes(index, sentiment, fill = article)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ article, ncol = 2, scales = "free_x")
```

##Comparing the three sentiment dictionaries

```{r,warning=FALSE, message=FALSE}
sentidoc1 <-  sentitext %>%
  filter(article == "doc1") 

afinnsenti <- sentidoc1 %>%
  inner_join(afinn, by = "word") %>%
  group_by(index = linenumber ) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN")

bingnrcsenti <- bind_rows(sentidoc1 %>% 
                            inner_join(bing) %>%
                            mutate(method = "BING"),
                          sentidoc1 %>% 
                            inner_join(nrc %>%
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = linenumber, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

bind_rows(afinnsenti, 
          bingnrcsenti) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

##Most common sentiment words

```{r,warning=FALSE, message=FALSE}
bing_counts <- sentitext %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_counts

bing_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

##Wordclouds
```{r,warning=FALSE, message=FALSE}
sentitext %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

library(reshape2)

sentitext %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("pink", "gray20"),
                   max.words = 100)
```

#Chapter 3 tf-idf

```{r,warning=FALSE, message=FALSE}
wordcount <- tidy_webtext %>%
  group_by(article) %>%
  count(article, word) %>%
  summarise(total = sum(n)) 

tidy.text <- left_join(tidy_webtext %>%
  group_by(article) %>%
  count(article, word)%>%
  ungroup(), 
  wordcount)
```

```{r,warning=FALSE, message=FALSE}
ggplot(tidy.text, aes(n/total, fill = article)) +
  geom_histogram(show.legend = FALSE) +
  #xlim(NA, 0.01) +
  facet_wrap(~ article, ncol = 2, scales = "free_y")
```

```{r}
# freq_by_rank <- tidy.text %>% 
#   group_by(article) %>% 
#   mutate(rank = row_number(), 
#          `term frequency` = n/total) %>%
#   arrange(desc(`term frequency`))

freq_by_rank <- tidy.text %>% 
  group_by(article) %>% 
  mutate(`term frequency` = n/total) %>%
  arrange(desc(`term frequency`)) %>%
  mutate(rank = row_number()) 
```

```{r}
rank_subset <- freq_by_rank %>% 
  filter(rank < 100,
         rank > 10)

rankreg <- lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
rankreg$coef
```

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = article)) + 
  geom_abline(intercept = rankreg$coef[1], slope =rankreg$coef[2], color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

###tf-idf function

```{r,warning=FALSE, message=FALSE}
tidytext <- tidy.text %>%
  bind_tf_idf(word, article, n) %>%
  select(-total) %>%
  arrange(desc(tf_idf))
tidytext

tidytext %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(article) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = article)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~article, ncol = 2, scales = "free") +
  coord_flip()
```


