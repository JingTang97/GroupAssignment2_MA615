---
title: "correlaid_text_analysis"
author: "Xiang XU, Jing(Mira) Tang, Ningze(Summer) ZU, Jianhao(Miller) Yan"
date: "November 3, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(
  "ggplot2",
  "knitr",
  "arm",
  "data.table",
  "car",
  "rvest",
  "RCurl",
  "RTidyHTML",
  "XML",
  "htm2txt",
  "dplyr",
  "tidytext",
  "stringr",
  "tidyr",
  "scales",
  "wordcloud"
)
```


##Scraping webpages
```{r, echo=FALSE}
url1 <- "https://correlaid.org/blog/posts/understand-p-values"
url2 <- "https://correlaid.org/blog/posts/blockchain-explained"
url3 <- "https://correlaid.org/blog/posts/music-with-r"

doc1 <- gettxt(url1)
# doc11 <- getURL(url1)
# dic11 <- htmlTreeParse(doc11)
# doc1111 <- readLines(url1)
doc2 <- gettxt(url2)
doc3 <- gettxt(url3)

raw.text <- as.data.frame(rbind(cbind("doc1",doc1),
                                cbind("doc2",doc2),
                                cbind("doc3",doc3)))
colnames(raw.text)  <-  c("article", "text")  
raw.text$article <- as.character(raw.text$article)
raw.text$text <- as.character(raw.text$text)
#str(raw.text)
```

##tidy text
```{r, echo=FALSE}
tidy_webtext <- raw.text %>%
  # unnest_tokens(sentence, text, token = "sentences") %>% 
  unnest_tokens(word, text)%>% 
  mutate(word = str_extract(word , "[a-z'\\s]+")) %>%
  anti_join(stop_words, by= "word") %>%
  filter(!word %in% "[\\s]+",
         !word %in% "",
         !word %in% NA)
```


##look at single word frequency and visualize
###first at doc1
```{r, echo=FALSE}
text1_count <- tidy_webtext %>%
  filter(article == "doc1" )%>%
  count(word, sort = TRUE) 

text1_count %>%
  filter(n > 10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() 
```

###plotting and comparing the three articles
```{r,warning=FALSE, message=FALSE, echo=FALSE}
frequency <- tidy_webtext %>% 
  count(article, word) %>%
  group_by(article) %>%
  mutate(proportion = n/sum(n) )%>%
  select(-n) %>%
  spread(article, proportion) %>% 
  gather(article, proportion, 'doc2' : 'doc3') 
#fill all NA (word proporton) with zero
frequency[is.na(frequency)] <- 0
  

ggplot(frequency, aes(x = proportion, y = `doc1`, color = abs(`doc1` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = .7, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.01), low = "darkslategray4", high = "gray75") +
  facet_wrap(~ article, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "doc1", x = NULL)
  
```

```{r, echo=FALSE}
cor.test(data = frequency[frequency$article == "doc2",],
         ~ proportion + `doc1`)

cor.test(data = frequency[frequency$article == "doc3",],
         ~ proportion + `doc1`)
```

*As we saw in the plots, the word frequencies have little frequencies in three articles.*

#Sentiment analysis

```{r, echo=FALSE}
afinn <- get_sentiments("afinn")
bing <- get_sentiments("bing")
nrc <- get_sentiments("nrc")
```

###bing sentiment analysis
```{r, echo=FALSE}
sentitext <-  raw.text %>%
  unnest_tokens(sentence, text, token = "sentences") %>% 
  group_by(article) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, sentence) %>%
  anti_join(stop_words, by = "word")

bing_analysis <- sentitext %>%
  inner_join(bing, by = "word") %>%
  count(article, index = linenumber , sentiment)%>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(bing_analysis, aes(index, sentiment, fill = article)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ article, ncol = 2, scales = "free_x")
```

##Comparing the three sentiment dictionaries

```{r,warning=FALSE, message=FALSE, echo=FALSE}
sentidoc1 <-  sentitext %>%
  filter(article == "doc1") 

afinnsenti <- sentidoc1 %>%
  inner_join(afinn, by = "word") %>%
  group_by(index = linenumber ) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN")

bingnrcsenti <- bind_rows(sentidoc1 %>% 
                            inner_join(bing) %>%
                            mutate(method = "BING"),
                          sentidoc1 %>% 
                            inner_join(nrc %>%
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = linenumber, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

bind_rows(afinnsenti, 
          bingnrcsenti) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

##Most common sentiment words

```{r,warning=FALSE, message=FALSE, echo=FALSE}
bing_counts <- sentitext %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_counts

bing_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

##Wordclouds
```{r,warning=FALSE, message=FALSE, echo=FALSE}
sentitext %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

```{r,warning=FALSE, message=FALSE, fig.width=6, fig.height=6, echo=FALSE}

library(reshape2)

sentitext %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c( "gray20","pink"),
                   max.words = 100)
```

#Chapter 3 tf-idf

```{r,warning=FALSE, message=FALSE, echo=FALSE}
wordcount <- tidy_webtext %>%
  group_by(article) %>%
  count(article, word) %>%
  summarise(total = sum(n)) 

tidy.text <- left_join(tidy_webtext %>%
  group_by(article) %>%
  count(article, word)%>%
  ungroup(), 
  wordcount)
```

```{r,warning=FALSE, message=FALSE, echo=FALSE}
ggplot(tidy.text, aes(n/total, fill = article)) +
  geom_histogram(show.legend = FALSE) +
  #xlim(NA, 0.01) +
  facet_wrap(~ article, ncol = 2, scales = "free_y")
```

```{r,warning=FALSE, message=FALSE, echo=FALSE}
# freq_by_rank <- tidy.text %>% 
#   group_by(article) %>% 
#   mutate(rank = row_number(), 
#          `term frequency` = n/total) %>%
#   arrange(desc(`term frequency`))

freq_by_rank <- tidy.text %>% 
  group_by(article) %>% 
  mutate(`term frequency` = n/total) %>%
  arrange(desc(`term frequency`)) %>%
  mutate(rank = row_number()) 
```

```{r,warning=FALSE, message=FALSE, echo=FALSE}
rank_subset <- freq_by_rank %>% 
  filter(rank < 100,
         rank > 10)

rankreg <- lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
rankreg$coef
```

```{r,warning=FALSE, message=FALSE, echo=FALSE}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = article)) + 
  geom_abline(intercept = rankreg$coef[1], slope =rankreg$coef[2], color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

###tf-idf function

```{r,warning=FALSE, message=FALSE, echo=FALSE}
tidytext <- tidy.text %>%
  bind_tf_idf(word, article, n) %>%
  select(-total) %>%
  arrange(desc(tf_idf))
tidytext

tidytext %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(article) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = article)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~article, ncol = 2, scales = "free") +
  coord_flip()
```

#Chapter 4 n-grams and correlations

We use `unnest_tokens` function to tokenize the articles into consecutive sequences of words, called n-grams. Here we focus on bigrams, aka two consecutive words.

As one might expect, a lot of the most common bigrams are pairs of common (uninteresting) words, such as of the and to be: what we call “stop-words” . This is a useful time to use tidyr’s `separate()` and `unite()`, which splits a column into multiple based on a delimiter and reunite them. In this process we can remove cases where either is a stop-word.

Also, we clean the bigrams by `str_extract()` and `filter()` function to remove cases where either is NA, space or non-letter word.

Then we look at tf_idf of bigrams and visualize them.

```{r,warning=FALSE, message=FALSE, echo=FALSE}
tidy_bigram <- raw.text %>%
  # unnest_tokens(sentence, text, token = "sentences") %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)%>% 
  
  separate(bigram, c("word1", "word2", sep = " "))%>%
  mutate(word1 = str_extract(word1 , "[a-z'\\s]+")) %>%
  filter(!word1 %in% stop_words$word ,
         !word1 %in% "[\\s]+",
         !word1 %in% "",
         !word1 %in% NA) %>%
  mutate(word2 = str_extract(word2 , "[a-z'\\s]+")) %>%
  filter(!word2 %in% stop_words$word,
         !word2 %in% "[\\s]+",
         !word2 %in% "",
         !word2 %in% NA) %>%
  
  unite(bigram, word1, word2, sep = " ") %>%
  count( article,bigram, sort = TRUE) %>%
  bind_tf_idf( bigram, article,n) %>%
  arrange(desc(tf_idf))

head(tidy_bigram,10)
```

```{r, fig.width=14, fig.height=6, echo=FALSE}
tidy_bigram %>%
  group_by(article)%>%
  top_n(15, tf_idf)%>%
  ungroup()%>%
  mutate(bigram= reorder(bigram, tf_idf))%>%
  ggplot( aes(bigram, tf_idf, fill= article)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y ="tf-idf") +
  facet_wrap(~ article, ncol =3, scales = "free") +
  coord_flip()
```

Figure: The 12 bigrams with the highest tf-idf

There are advantages and disadvantages to examining the tf-idf of bigrams rather than individual words. Pairs of consecutive words might capture structure that isn’t present when one is just counting single words, and may provide context that makes tokens more understandable . However, the per-bigram counts are also sparser: a typical two-word pair is rarer than either of its component words. Thus, bigrams can be more useful when we have a larger text dataset.

* Using bigrams to provide context in sentiment analysis
```{r,warning=FALSE, message=FALSE, echo=FALSE}

negation_words <- c("not", "no", "never", "without")

# senti_bigram <- tidy_bigram %>%   
#   separate(bigram, c("word1", "word2", sep = " ")) %>%
#   filter(word1 %in% afinn$word ) %>%
#   inner_join(afinn, by = c(word2 = "word")) %>%
#   count(word1,word2, score, sort = TRUE) %>%
#   ungroup()
# 
# head(negsenti_bigram)

negsenti_bigram <- tidy_bigram %>%   
  separate(bigram, c("word1", "word2", sep = " ")) %>%
  filter(word1 %in% negation_words) %>%
  inner_join(afinn, by = c(word2 = "word")) %>%
  count(word1,word2, score, sort = TRUE) %>%
  ungroup()

head(negsenti_bigram)
```

For these are three academic articles and there are not many bigrams with negative terms. So we can skip this part.

* Visualizing a network of bigrams with ggraph

```{r,warning=FALSE, message=FALSE, echo=FALSE}
library(igraph)

count_bigram <- raw.text %>%
  # unnest_tokens(sentence, text, token = "sentences") %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)%>% 
  
  separate(bigram, c("word1", "word2", sep = " "))%>%
  mutate(word1 = str_extract(word1 , "[a-z'\\s]+")) %>%
  filter(!word1 %in% stop_words$word ,
         !word1 %in% "[\\s]+",
         !word1 %in% "",
         !word1 %in% NA) %>%
  mutate(word2 = str_extract(word2 , "[a-z'\\s]+")) %>%
  filter(!word2 %in% stop_words$word,
         !word2 %in% "[\\s]+",
         !word2 %in% "",
         !word2 %in% NA) %>%
  
  count(word1, word2, sort = TRUE)

head(count_bigram)

# filter for only relatively common combinations
bigram_graph <- count_bigram %>%
  filter(n > 2) %>%
  graph_from_data_frame()

bigram_graph

library(ggraph)
# set.seed(2019)

# ggraph(bigram_graph, layout = "fr") +
#   geom_edge_link() +
#   geom_node_point() +
#   geom_node_text(aes(label = name), vjust = 1, hjust = 1)
# 

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

* Counting and correlating pairs of words 

```{r,warning=FALSE, message=FALSE, echo=FALSE, fig.width=18, fig.height=6}
tidy_doc1 <- tidy_webtext %>%
  filter(article == "doc1")
head(tidy_doc1)

library(widyr)

# count words co-occuring within sections
word_pairs <- tidy_webtext %>%
  pairwise_count(word, article, sort = TRUE)
word_pairs
```

*  Pairwise correlation

Find the phi coefficient between words based on how often they appear in the same article. 

```{r,warning=FALSE, message=FALSE, echo=FALSE, fig.width=18, fig.height=6}


word_cors <- tidy_webtext %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word,article, sort = TRUE)
word_cors

unique(word_cors$item1)
unique(word_cors$item2)
```

Let's pick particular interesting words and find the other words most associated with them.

```{r,warning=FALSE, message=FALSE, echo=FALSE, fig.width=18, fig.height=6}

word_cors %>%
  filter(item2 %in% c("hypothesis", "blockchain", "difference", "values")) %>%
  group_by(item2) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item1 = reorder(item1, correlation)) %>%
  ggplot(aes(item1, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item2, scales = "free") +
  coord_flip()
```

Visualize the correlations and clusters of words.

```{r,warning=FALSE, message=FALSE, echo=FALSE, fig.width=6, fig.height=6}
set.seed(2016)
word_cors %>%
  filter(correlation > 0) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```
